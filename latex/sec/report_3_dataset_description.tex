\section{Dataset description}
It~\cite{yuanjibao} is highlights that the quality of existing plagiarism text datasets is generally low, characterized by issues such as uneven data distribution, limited corpus size, and a lack of diversity in plagiarized texts. These challenges not only constrain the training effectiveness of plagiarism detection models but also hinder their performance in handling complex plagiarism scenarios. To address these issues, the article proposes a solution: generating three types of data using GPT-3.5, including non-plagiarized texts, implicitly plagiarized texts, and explicitly plagiarized texts. This generation strategy significantly enhances the diversity and quality of the dataset by simulating different types of plagiarism behaviors.

Our research adopts this approach to construct a dataset. Specifically, Implicit Plagiarism refers to advanced forms of plagiarism where only the ideas, logic, or processes are copied, with significant rephrasing and restructuring of the text. Such cases are challenging to detect with traditional text-matching algorithms because the expressions may differ entirely from the original, despite retaining a high level of conceptual similarity. On the other hand, Explicit Plagiarism involves straightforward text reuse, such as changing variable names, substituting certain words, or slightly rearranging sentence structures. This form of plagiarism is more direct and can often be detected using similarity search or pattern-matching algorithms.

By incorporating these three distinct types of data, our dataset becomes more comprehensive and capable of effectively training models to handle a wide range of plagiarism scenarios, from superficial to deeply embedded plagiarism. This approach not only addresses the issue of low-quality datasets in the field of plagiarism detection but also lays the foundation for building more robust detection systems. We believe that this improved data generation strategy can support future research and further advance plagiarism detection technology.

We have now generated 150 data samples, including 60 samples of explicit plagiarism, 60 samples of implicit plagiarism, and 30 samples of non-plagiarism. We will increase the amount of generated data later based on the model's performance and training outcomes.